# ICML2025_ReasonFlow_Rebuttal

## Response to Reviewer GpCX
### Q1. Generalize to other base models and relaate to current Long-CoT work.

#### Table.1
|Task| MATH500 | AIME2024 | AMC2023 | Olympiad Bench | Gaokao En 2023 |
|-|-|-|-|-|-|
|ReasonFlow-Qwen2.5-32B|88.6|46.7|80.0|57.6|79.6|

| Introduction | Actually we have tried other base models such as Qwen2.5-32B-Insturct since it is widely adpoted by similar works. It also shows good performance compare to base models, here **we present the result in the Table.1**: |
| ------------ | ------------------------------------------------------------ |
| **Analyze**  | It has also achieved strong performance compare to other models, **which showcase that our method could well generalized to other base models**. But the Qwen-based version cannot outperform our current version of ReasonFlow which is finetuned based on QwQ. For better presentation, in the size of 32B model, we choose to use the QwQ version in our paper. |

#### Table.2
|Task|ReasonFlow-DS-32B@pass1|ReasonFlow-Qwen-32B@pass1|R1-Distill-32B@pass1| OpenAI o1-mini@pass1|LIMO -32B@pass1|s1-32B@pass1|
|-|-|-|-|-|-|-|
|MATH500|**96.0**|94.6|94.3|90.0|90.6|84.8|
|AIME2024|**76.67**|66.67|72.6|56.7|50.0|36.0|
|AIME2025|**53.33**|46.67|46.67|50.8|37.2|26.7|
|GPQA-Diamond|**67.17**|63.13|62.1|60.0|65.2|59.6|

| Introduction | Thank you for your insightful advice about the relation with our work and Long-CoT. Actually, after Long-CoT work emerged since January this year, we have continued to explore the possible ways of application of ReasonFlow related to Long-CoT models. In our initial attempts, we utilized the reasoning trajectories generated from ReasonFlow on s1k dataset[1] and transformed them into Long-CoT type by QwQ and using simple supervised finetuning method to train our Long-CoT reasoning LLM from scratch like s1[1] and LIMO[2]. In response to your concern about the choose of base models, here we choose two different base LLM, including **Long-CoT models like DeepSeek-R1-Distill-Qwen-32B** and **general models like Qwen2.5-32B-Instruct**. The results are quite promising and our long-CoT models even achieve SOTA level performance compare to other Long-CoT models. **We present the results in Table.2**|
| ------------ | ------------------------------------------------------------ |
| **Analyze**  |As you can see, to better align with the current trends for more comprehensive evaluation on challenging benchmarks, we further add recent AIME2025 and evaluate on wider range of reasoning tasks like GPQA-Diamond which includes more science topics other than math. As shown in the table, both version of our Long-CoT type models (ReasonFlow-DS-32B, finetuned based on R1-Distill-32B, and ReasonFlow-Qwen-32B, finetuned based on Qwen2.5-32B-Instruct) outperform current Long-CoT works and achieve significant gains in reasoing accuracy, which further proofs that our method is solid and could even produce better reasoning trajectories that could be used as the base reasoning flow to build a high-quality long-cot dataset to train powerful reasoning models.|



### Q5. The number of tokens for each task.
#### Table.3
| Model                | AMC2023 | MATH500 | AIME2024 | Olympiad Bench | GaoKao En 2023 |
| -------------------- | ------- | ------- | -------- | -------------- | -------------- |
| Qwen2.5-32B-Instruct | 327.18  | 346.176 | 497.96   | 562.47         | 275.68         |
| QwQ-32B-Preview      | 1237.4  | 1532.6  | 4873.5   | 4726.4         | 2104.8         |
| ReasonFlow-32B       | 1158.9  | 1398.4  | 4275.6   | 4189.5         | 1867.5         |

| Introduction | Here we utilize the average word numbers within the reasoning process generated by the LLMs. As the table illustrates, our ReasonFlow-32B consistently utilizes fewer tokens on average compared to the QwQ-32B-Preview model across all evaluated mathematical reasoning benchmarks. For instance, on the MATH500 dataset, ReasonFlow achieves its results using approximately 8.8% fewer tokens than QwQ, and on AIME2024, the reduction is around 12.3%. **We present the results in Table.3**|
| ------------ | ------------------------------------------------------------ |
| **Analyze**  |This reduction in token usage directly supports one of the core contributions highlighted in our paper and acknowledged by the reviewer: ReasonFlow's ability to achieve a more efficient exploration-exploitation trade-off. By leveraging structured thought templates and optimizing the selection of reasoning steps, ReasonFlow can navigate the problem-solving space more effectively, reaching high-quality solutions with less computational overhead (represented here by token count) compared to the QwQ approach. |

## Response to Reviewer tQLU
### Q1. More ablation studies on the SFT stage and RL stage.
#### Table.4
|Model|Structure Accuracy|Retrieving Accuracy|
|-|-|-|
|Qwen2.5-32B-Instruct|47.8%|32.1%|
|Qwen2.5-32B-Instruct-SFT|**87.6%(+39.8%)**|**82.4%(+50.3%)**|
|QwQ-32B-Preview|42.6%|36.4%|
|QwQ-32B-Preview-SFT|**89.3%(+46.7%)**|**96.5%(+60.1%)**|

| Introduction | For SFT stage, we aim to incorporate the LLM with the ability of conducting structtued RAG and have a in-dpeth comprehension of the templates in the library. Based on that, we conduct additional ablation experiment on SFT stage. Here we randomly sampled 100 thought templates along with the corresponding example problems, and utilize o3-mini to generate 10 variant problems for each example and make sure that they differed from the original examples while still assessing the same underlying knowledge and skills as we did in Appendix.B. Then we evaluate the average retrieving accuracy among the base models and our SFT models. Here we define two metrics to comprehensively evaluate the structued RAG ability gained after the SFT stage. The structure accuracy is the percentage that the generated conetent has the correct structued format that could be correctly parsed and continue to the retrieval phase, the retrieving accuracy is the percentage that the models correctly retrieve the corresponding template for the problem after successfully parsed the structured query. **We present the results in Table.4**|
| ------------ | ------------------------------------------------------------ |
| **Analyze**  | We can find that through our first stage SFT, both structure accuracy and retrieving accuracy has significantly improved, which means that the first stage SFT could help to improve the structured output of LLMs which allows for our more accurate structured RAG, and the retreiving accuracy also improved which means that the models has a deeper understanding of the relationship between the problem and the corresponding templates. |

#### Table.5
|Model|Structure Accuracy|Retrieving Accuracy|Reasoning Accuracy|
|-|-|-|-|
|Qwen2.5-32B-Instruct-SFT|87.6%|82.4%|72.1%|
|Qwen2.5-32B-Instruct-SFT+RL|**91.4 (+3.8%)**|**85.6% (+3.2%)**|**78.4% (+6.3%)**|
|QwQ-32B-Preview-SFT|89.3%|96.5%|82.5%|
|QwQ-32B-Preview-SFT+RL|**94.7 (+5.4%)**|**97.4% (+0.9%)**|**89.7% (+7.2%)**|

| Introduction | For the RL stage, we aim to utilize the reward signal from downstream reasoning tasks to optimize the ability of LLMs to generate correct and logical reasoning trajecotris. Here we conduct an intutive ablation experiment to demonstrate the improvements and effectiveness of template trajectories rewarding stage. We sampled 100 example problems and utilzie the model after the SFT stage and the model that experienced both SFT and RL stage to retrieve thought templates and genrate corresponding reasoning trajectories as high-level guidelines for inference LLM to instiate. Here we add another metric which is the reasoning accuracy of the infernce LLM that reflects the quality of the reasoning trajectories, **we present the results in Table.5** |
| ------------ | ------------------------------------------------------------ |
| **Analyze**  | We can see that after the RL stage, all three metrics has improved, especially the reasoning accuracy of the downstream inference LLMs. It shows that the RL stage could effectively improve the quality of generated reasoning trajectories thus improving the final reasoning accuracy. What's more, we observe that the structure accruacy and the retrieving accuracy has also increased. We attribute this to the fact that the RL stage not only reinforces the model's ability to generate logically coherent reasoning trajectories but also implicitly encourages adherence to the desired structured format. In other words, by optimizing the reward signal based on downstream reasoning performance, the model is driven to produce outputs that are **both logically consistent and structurally well-formed**. This dual reinforcement helps reduce format errors and improves template retrieval, ultimately resulting in enhanced overall performance. |
| **Summary**  |In summary, while plan-and-solve prompting methods may suffice for simpler tasks, **our training approach with ReasonFlow is vital for addressing complex problems**. It not only augments the inherent reasoning capabilities of LLMs with external structured knowledge but also systematically improves the quality of reasoning trajectories through two-stage training. These advantages are empirically validated by our ablation studies as above, which underscore the necessity and effectiveness of our additional training stages.|

## Response to Reviewer 49yc
### Q1. The choice for base LLMs and modifaction of our current paper.
#### Table.6
|Model|Without Template(%)|With Template(%)|
|-|-|-|
|o1-mini-2024-09-12|89.2%|99.3% **(+10.1%)**|
|gemini-2.0-flash-thinking-exp-1219|84.6%|98.7% **(+14.1%)**|

| Question                                                     | Explain                                                      |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1). The choice of QwQ as our base model and the highlight of our paper | We really appreciate your thoughtful advice regarding the presentation. Initially, we emphasized our results to highlight the effectiveness of our template-augmented reasoning framework. Our base models, such as QwQ-Preview, already exhibit strong performance (outperforming o1 on MATH and showing comparable results on other datasets). In light of your feedback, we will revise our wording and presentation to clarify that our approach builds upon these robust base models. Our goal is to demonstrate that fine-tuning QwQ with our framework not only leverages its inherent strengths but also provides a cost-effective and scalable mechanism via template retrieval and interplay that enhances reasoning capabilities and reasoning efficiency. We will adjust the manuscript to ensure a balanced comparison that accurately reflects these contributions without over-advertising our results. What's more, we have tried other base models such as Qwen2.5-32B-Insturct since it is widely adpoted by similar works. It also shows good performance compare to base models. Due to page limits, **please refer to our reponse to Reviewer GpCX's Q1.** |
| 2). The additional results in Appendix.B should be include in the main paper |  We really appreciate your recongnition for our results in Appendix.B. However, due to the page limits, we fail to include the experiment in our current manuscript. We will follow your insightful suggestions and make changes to our current arrangements to leave space for the results in Appendix.B in our final version. Here we also add the test results including reasoninig LLMs like o1-mini and Gemini 2.0 - flash-thinking to make the comparison **more comprehensive in Table.6.** We can see that even for powerful reasoning LLMs like o1-mini and Gemini, our template could still provide insight for them to solve challenging problems. |
| 3). Adding some essential references along with the clear version number of the model | We carefully checked the paper you mentioned and we found that these paper are actually related to our work in different ways, we will update our current manuscript and add these essential references to our paper. We think these will make our paper cover a more comprehensive comparison between different methods.  As for the version number of the models include in our paper, for o1-sereis, we use **o1-mini-2024-09-12** along with **o1-preview-2024-09-12**, for gpt-4o series, we use **gpt-4o-2024-11-20** and for cluade models, we use **claude-3-5-sonnet-20241022**. We sinceely appreciate your insightful suggestions and we will definitely update our manuscript to make it better. |
| 4). $\pi_{sft} $ is used without reference| We sincerely thank you for pointing it out. In the equation.4 we use $\pi_{sft}$ to substitute $\pi_{struct}$, which is a more noraml expression of the reference model in the loss function of DPO. Since we have metioned that $\pi_{\theta}$ is initialized from $\pi_{struct}$. Based on that we asssume it is intitutive that $\pi_{sft}$ represents $\pi_{struct}$ and not additionally make more explaination for it. We will add the explaination for $\pi_{sft}$ in the final version of our manuscript to address your concern. |


### Q4. Adding AIME2025 results.

#### Table.7
| Model          | MATH500 | AIME2024 | AIM2025 | GPQA-Diamond |
| -------------- | ------- | -------- | ------- | ------------ |
| Qwen2.5-32B-Instruct     | 84.2% | 16.67%   | 26.67% | 48.48%    |
| ReasonFlow-Qwen2.5-32B | 88.6% | 46.7%    | 33.33% | 57.2% |
| QwQ-32B-Preview |    90.6%     |    50.0%     | 33.3% | 57.58% |
| ReasonFlow-32B | 91.2%   | 56.7%    | 37.2%   | 61.2%        |

| Introduction | We have added the results of AIME2025 to the table above for more comprehensive evaluation in Table.7 |
| ------------ | ------------------------------------------------------------ |
| **Analyze**  | These results clearly show that our method consistently outperforms the corresponding base models across all benchmarks. In particular, on AIME2025, ReasonFlow improves performance by a significant margin over the base model. This confirms that our approach through its structured template library and two-stage training enhances reasoning capabilities even on new, unseen datasets. Moreover, the improvements on GPQA-Diamond illustrate the versatility of our framework across diverse task domains.|


## References:

[1] Muennighoff N, Yang Z, Shi W, et al. s1: Simple test-time scaling[J]. arXiv preprint arXiv:2501.19393, 2025.

[2] Ye Y, Huang Z, Xiao Y, et al. LIMO: Less is More for Reasoning[J]. arXiv preprint arXiv:2502.03387, 2025.
